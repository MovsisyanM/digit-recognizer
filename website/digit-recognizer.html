<html>
  <head>
    <title>MSXCN - Movsisyan.info</title>
</head>
<body>
    <h1>Movsisyans Singularity eXtracting Convolutional Network</h1>
    <h6>by <a href="https://movsisyan.info/?redir=https://www.linkedin.com/in/movsisyaninfo/">Mher Movsisyan</a> and <a href="https://movsisyan.info/?redir=https://www.linkedin.com/in/tigran-avetisyan/">Tigran Avetisyan</a></h6>
    <p>
        Hey there! Thank you for checking out my project. 
        We made a cool digit recognizer, and it will try to guess 
        which digit you have drawn on this canvas. Want to give it a shot?
    </p>
    <div id="canvasHolder">
        <canvas id="inputCanvas"></canvas>
    </div>
    <p id="prediction">Draw in the black square to test the model</p>

    <h2>How it does the magic - from your click to a prediction</h2>
    <p>
        What you see in front of you is your browser's visualization of my website. Your browser has asked Movsisyan's server (a computer that holds and serves Movsisyan's files) for the necessary information to recreate everything you see on your screen. In response to your browser's humble request, Movsisyan's server responds with four types of files: HTML, CSS, Javascript, and Pictures/Illustrations. 
        <br/>
        <br/>
        The first file is of type <b>HTML</b>, which stands for Hyper-Text Markup Language. The contents of this file instruct the browser to generate the text and structure of the website you see currently.
        <br/>
        The second file is of type <b>CSS</b>, which stands for Cascading Style Sheets. The contents of this file tell your browser the details about the elements in the HTML file, for example, how big the text should be, which font to use to display it, what colors to use.
        <br/>
        The third file is of type <b>JavaScipt</b>, which is a programming language. It acts as the logic behind the website; it tells the browser what to do when you click and drag your mouse, how to talk to Mr. MSXCN, and ask for a prediction.
        <br/>
        The fourth and final files are pictures or illustrations that need to be displayed wherever Mr. HTML instructed for them to be displayed.
        <br/>
        <br/>
        Once the browser has displayed the website, you start reading the text within, eventually stumbling upon the black square. The moment you click within it, Mr. Javascipt starts filling the space with black pixels. The moment you let go of the mouse button, Mr. Javascript rushes to center the drawing and sends it to Mr. Server, asking him to predict which digit you drew, and assuming it was, in fact, a digit that you drew.
        <br/>
        <br/>
        Upon receiving the request, Mr. Server scales the picture down and gives it to Mr. MSXCN, who gets excited and tries to predict the digit correctly. Mr. Server does not tell Mr. MSXCN whether or not the prediction was correct since the only way to know if it is, is to ask Mr. Javascript, who then will ask you. You are honest and trustworthy, but some people are not and will jokingly say it is a "5" that they drew when in fact, they drew an "8." This would upset Mr. MSXCN as he would think that his prediction was wrong, making him question his entire life. We do not want that to happen :)
        <br/>
        <br/>
        Mr. Server responds to Mr. Javascript with the digit that was predicted. Mr. Javascript asks Mr. Browser to ignore a part of the old instructions given to him by Mr. HTML and instead display the prediction, to which Mr. Browser kindly agrees.
    </p>
    <h2>How does Mr. MSXCN predict?</h2>
    <p>Mr. MSXCN is complex indeed, but do not worry! Once we look at the individual layers one by one, everything will be clear. As illustrated in the image below, a Preprocessing block, a Convolutional chain, a Singularity Extracting chain, and a Cognitive block compose MSXCN.
        <br/>
        <br/>
        <img src="visuals\MSXCN Structure.svg" class="svg"></img>
        <br/>
        <br/>
        The <b>Preprocessing block</b> prepares the data for consumption. During training, the data passes through five consecutive layers: Reshape, RandomWidth, RandomTranslation, RandomZoom, Resizing. However, during evaluation or production, the data only passes through the Reshape layer.
        <br/>
        <br/>
        <img src="visuals\Preprocessing block.svg" class="svg"></img>
        <br/>
        <br/>
        The <b>Reshape</b> layer reshapes the 2D tensor of size (n, 784) into (n, 28, 28, 1), n being the number of observations per batch. The RandomWidth layer scales the image along the x-axis by a random amount. The RandomTranslation layer translates the image to a random location. The RandomZoom layer zooms the image by a random amount. The Resizing layer resizes the image back to 28x28. The data is then sent off to be processed by the Convolutional and Singularity Extracting chains.
        <br/><br/>
        The <b>Convolutional Chain</b> is designed to learn and detect patterns that correspond to each digit. It, as illustrated below, is built with 11 building blocks: 3 convolutional blocks, 3 batch norm., 3 dropout layers, a global average pooling layer, and a quite redundant but still useful flattening layer. 
        <br/>
        <br/>
        <img src="visuals\ConvChain.svg" class="svg"></img>
        <br/>
        <br/>
        The <b>dropout</b> layer discards some of the connections between layers, thus countering overfit. The batch normalization layer standardizes its inputs. The standardization of inputs helps the model learn faster as it does not have to spend numerous iterations adjusting each of the weights to account for extreme input values. The global average pooling layer maps the average of each spatial feature to a category confidence map which significantly reduces computational needs. 
        <br/>
        <br/>
        <img src="visuals\dropout.png"></img>
        <br/>
        <br/>
        The <b>flatten</b> layer assures that all output from the chain is flattened.
        <br/>
        <br/>
        <img src="visuals\Flatten.svg" width="70%"></img>
        <br/>
        <br/>
        The <b>Convolutional Block</b> is a series of sequentially connected convolutional, batch normalization, and max-pooling layers. During the instantiation of a block, we supply arguments upon which it generates and connects layers. Specifying the depth parameter causes the block to generate just as many "Convolution-Batch Norm" pairs, only leaving the last convolutional layer without a batch norm partner. If the pool argument is True, it generates a max-pooling layer of second degree; otherwise, it increases the strides of the last convolutional layer to 2. Below is a convolutional block with depth 2 and pooling enabled.
        <br/>
        <br/>
        <img src="visuals\Sample ConvBlock diagram.svg" class="svg"></img>
        <br/>
        <br/>
        The <b>convolutional</b> layer applies a convolution operation (basically a dot product) on the input matrix using kernels. The weights of the kernels are inferred during training which allows the model to find useful repeating spatial patterns in the inputs. The animation below shows how the kernel (dark 3x3 area) passes above each input pixel (blue squares), applying a convolution operation to nearby pixels, thus computing the output (green matrix).
        <br/>
        <br/>
        <img src="visuals\conv example.gif"></img>
        <br/>
        <br/>
        The max-pooling layer helps us reduce computational needs by down-sampling the input. 
        <br/>
        <br/>
        <img src="visuals\maxpool.gif"></img>
        <br/>
        <br/>
        <br/>
        <br/>
        <img src="visuals\Singularity Extracting chain.svg" class="svg"></img>
        <img src="visuals\SE chain outputs.svg" class="svg"></img>
        The <b>Singularity eXtractor</b> layer accentuates non-uniform feature localities that otherwise a convolutional block might miss. An example of this would be the array [10, 10, 2, 10, 10], where the feature in the middle differs from the rest significantly, this significant difference would be accentuated by the singularity extractor, and it would output an array that would look like this: [0.13, 0.08, 0.94, 0.08, 0.13]. The formula below defines the Singularity Extracting operation of kernel size 3x3.
        <br/>
        <br/>
        <img width="650" src="https://latex.codecogs.com/png.latex?%5Cdpi%7B240%7D%20%5Cbg_white%20%5Cbegin%7Bpmatrix%7D%5Cfrac%20%7BConvolution(Input%2C%20%5Cbegin%7Bbmatrix%7D1%20%26%201%20%26%201%20%5C%5C1%20%26%201%20%26%201%20%5C%5C1%20%26%201%20%26%201%20%5C%5C%20%5Cend%7Bbmatrix%7D)%5C%20-%5C%20Input%7D%20%7BConvolution(Input%2C%20%5Cbegin%7Bbmatrix%7D1%20%26%201%20%26%201%20%5C%5C1%20%26%201%20%26%201%20%5C%5C1%20%26%201%20%26%201%20%5C%5C%20%5Cend%7Bbmatrix%7D)%7D%5Cend%7Bpmatrix%7D%5E%7Bdegree%7D">
        <br/>
        <br/>
        The <b>Cognitive Block</b>, as shown below, consists of concatenate, dense, batch norm, and dropout layers. The concatenate layer merges the outputs of the Convolutional and Singularity Extracting chains. The dense layers are just a bunch of linear functions with a  touch of non-linearity (in this case, a leaky rectified linear unit, tanh, and softmax).
        <br/>
        <br/>
        <img src="visuals\Cognitive Block.svg" class="svg"></img>
        <br/>
        <br/>
    </p>
    <h2>Training MSXCN</h2>
    <p>
        Two datasets were used to train and validate the model, but only one to test it. The larger dataset was the MNIST digits dataset containing 70,000 handwritten digits in total. 28,000 observations were used to test; the rest were merged with the second dataset. The second dataset is the Digits mini dataset containing 5500 digits drawn on the canvas you saw before (the black square). We collected and published this dataset; it is available for free on Kaggle (link below). The joint dataset was then split into 90% training and 10% validation sets. 
        <br/>
        <br/>
        The model was optimized using an RMSprop optimizer. A callback of ReduceLROnPlateu was implemented to help the model converge faster by decreasing the learning rate by a factor of 3 every 3 epochs of no improvement. A ModelCheckpoint was used to save the model every time it reached a new extremum. The custom-written GateOfLearning callback was used to kick the model out of local extrema, hoping it would converge to a better one. In the illustration below, we can see how the model can sometimes get stuck in local extrema. The goal of GateOfLearning is to kick the model into the air and hope it lands in better extrema. This goal is often not met. 
        <img src="visuals\converge.svg" width="40%"></img>
        Moreover, GateOfLearning can cause under/overfit on many occasions. It can also cause the learning rate to diverge into infinity (which is moderated by an exception raise) when the patience and factor values overpower the optimizer and ReduceLROnPlateau combined. However, in the right hands and with a pint of luck, it might push the model to a global extremum.
    </p>
</body>
<link rel="stylesheet" type="text/css" href="digit-recognizer.css">
<script src="digit-recognizer.js"></script>
</html>
